{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Abgegeben von (Name, Vorname):</span> \n",
    "Goxhufi, Driton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vorbereitung\n",
    "\n",
    "\n",
    "Wie immer müssen wir zuerst das NLTK-Modul laden. Außerdem importieren wir das Brown Corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Morphologie\n",
    "\n",
    "(Hinweis: Eine ausführlichere Kurzeinleitung gibt es in den beigestellten Folien (`skript-morphologie.pdf`).\n",
    "\n",
    "Die Morphologie innerhalb der Linguistik beschäftigt sich mit dem **inneren Aufbau von Wortformen** anhand von Morphemen.\n",
    "\n",
    "Es gibt verschiedene Arten von **Morphemen**:\n",
    "- **Wurzel**: der kleinste Teil eines Wortes, der die zentralen semantischen Informationen beiträgt und kein Affix enthält.\n",
    "    - z.B. *zer-**leg**-en*\n",
    "- **Derivationsaffixe**: eine geschlossene Klasse von Morphemen, die die Wortart (Nomen, Verb, Adjektiv, ...) bestimmen können\n",
    "    - z.B. **zer**-*leg-en*\n",
    "- **Stamm**: der Teil eines Wortes, der die Wurzel und alle Derivationsaffixe enthält.\n",
    "    - z.B. **zer-leg**-*en*\n",
    "- **Flexionsaffixe**: eine geschlossene Klasse von Morphemen, die immer an letzter Stelle im Wort stehen und bestimmte semantische (Genus, Komparation, Person, Numerus, Aspekt, Aktionsart, Tempus, Modus) oder funktionale (Kasus) Informationen beitragen.\n",
    "    - z.B. *zer-leg*-**en**\n",
    "\n",
    "Unter Normalisierung im Bereich der Morphologie versteht man die Ersetzung einer Wortform durch ihren Stamm oder durch eine Zitationsform (oder Lemma).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warum Normalisierung?\n",
    "\n",
    "- Wenn wir wissen wollen, wie groß der Wortschatz eines Textes ist.\n",
    "- Wenn wir wissen wollen, wie oft bestimmte Begriffe in einem Text vorkommen.\n",
    "    - z.B. Information Retrieval, Textklassifizierung\n",
    "- Wenn wir Suchanfragen verallgemeinern möchten.\n",
    "- ...\n",
    "\n",
    "Die Normalisierung ist besonders interessant bei Sprachen mit reicher Flexion. Dazu gehört das Deutsche."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalisierung der Groß-/Kleinschreibung\n",
    "\n",
    "Die einfachste Form der Normalisierung ist die Vereinheitlichung der Groß-/Kleinschreibung.\n",
    "\n",
    "Zum Beispiel kann man durchgängig die Kleinschreibung verwenden. Dafür steht die Standardfunktion `lower()` zur Verfügung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'peter went to the white house and read the new york times.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = \"Peter went to the White House and read the New York Times.\"\n",
    "word.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Problem bei dieser rasenmäherartigen Normalisierung besteht darin, dass gewisse Unterscheidungsmöglichkeiten hinsichtlich der Bedeutung verloren gehen können.\n",
    "\n",
    "- *White House* $\\Rightarrow$ *white house* \n",
    "- *Peter* $\\Rightarrow$ *peter*\n",
    "\n",
    "Besser ist es, nur das erste Wort eines Satzes klein zu schreiben $-$ aber auch nur dann, wenn das Wort sonst irgendwo im Korpus klein geschrieben wird. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">Aufgaben I</span>\n",
    "\n",
    "<span style=\"color:red\">A1:</span> Schreiben Sie eine Funktion `conditional_lower(input,corpusSentsTails)` die, ein Wort `word` in Kleinschreibung ausgibt, falls `word` in `corpusSentsTails` klein geschrieben wird.\n",
    "\n",
    "Nehmen Sie der Einfachheit halber an, dass `corpuSentsTails` diejenigen Wortformen aus dem Brown Corpus (Textsorte \"news\") enthält, die **nicht** satz-initial stehen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lösung zu A1:\n",
    "def conditional_lower(input, corpusSentsTails):\n",
    "    lowWord = [];\n",
    "    for i in corpusSentsTails[1:]:\n",
    "        if input in i:\n",
    "            print(i.lower())\n",
    "    return i.lower();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "noninitialSet = brown.words(categories=['news'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[conditional_lower(word,noninitialSet) for word in [\"That\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier ein Testset für `conditional_lower()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "petersburg\n",
      "petersburg\n",
      "peter\n",
      "peter\n",
      "peter\n",
      "peter\n",
      "peter\n",
      "peterson\n",
      "peterson\n",
      "peterson\n",
      "peterson\n",
      "peter\n",
      "peter\n",
      "richmond-petersburg\n",
      "that\n",
      "that\n",
      "that\n",
      "that\n",
      "that's\n",
      "that\n",
      "that\n",
      "that's\n",
      "that\n",
      "that\n",
      "that's\n",
      "that\n",
      "that\n",
      "that\n",
      "that\n",
      "that\n",
      "that\n",
      "that\n",
      "that\n",
      "that\n",
      "that\n",
      "that\n",
      "that\n",
      "that\n",
      "that's\n",
      "that's\n",
      "that\n",
      "that\n",
      "that\n",
      "that\n",
      "that\n",
      "that\n",
      "times\n",
      "times-picayune\n",
      "times\n",
      "white\n",
      "white\n",
      "white\n",
      "white\n",
      "white\n",
      "white\n",
      "white\n",
      "white\n",
      "white\n",
      "white\n",
      "whitey\n",
      "white\n",
      "white\n",
      "white\n",
      "white\n",
      "white\n",
      "white\n",
      "white\n",
      "white\n",
      "white\n",
      "white\n",
      "white\n",
      "white\n",
      "white\n",
      "white\n",
      "white\n",
      "white\n",
      "white\n",
      "white\n",
      "white\n",
      "white\n",
      "white\n",
      "white\n",
      "white\n",
      "white\n",
      "white\n",
      "white\n",
      "white\n",
      "white\n",
      "white\n",
      "white\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['!', '!', '!', '!', '!', '!']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[conditional_lower(word,noninitialSet) for word in [\"Pizza\", \"Peter\", \"That\", \"Times\", \"White\", \"Urlaubsantrag\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming (Rückschnitt)\n",
    "\n",
    "Als Stemming bezeichnet man Verfahren, die die Suffixe eines Wortes tilgen und dadurch unterschiedliche Wortformen vereinheitlichen: \n",
    "\n",
    "- *connect* $\\Rightarrow$ *connect*\n",
    "- *connected* $\\Rightarrow$ *connect*\n",
    "- *connecting* $\\Rightarrow$ *connect*\n",
    "- *connection* $\\Rightarrow$ *connect*\n",
    "- *connections* $\\Rightarrow$ *connect*\n",
    "\n",
    "Man beachte: \n",
    "- Solche Verfahren zielen **nicht** auf eine linguistisch adäquate Bildung des Stammes! Das ist Thema der Computationellen Morphologie bzw. Ziel des morphologischen Parsens. (siehe https://aclweb.org/aclwiki/Morphology_software_for_English)\n",
    "- Solche Verfahren können die Grenze zwischen Wortarten verwischen.\n",
    "- Der Übergang zwischen nützlichem und schädlichem Stemming ist fließend und auch abhängig von der Anwendung.\n",
    "    - *relate* $\\Rightarrow$ *relate*\n",
    "    - *relativity* $\\Rightarrow$ *relate*    (schädlich?)\n",
    "    - *organ* $\\Rightarrow$ *organ*  \n",
    "    - *organization* $\\Rightarrow$ *organ*    (schädlich!)\n",
    "    \n",
    "Die Funktionsweise ist vergleichsweise simpel: Das Stemming wird von bedingten Ersetzungsregeln der Form \n",
    "- `(condition) S1 -> S2`\n",
    "\n",
    "kontrolliert, wobei `S1` durch `S2` ersetzt wird, wenn `condition` erfüllt ist.\n",
    "\n",
    "Die Stemmingverfahren unterscheiden sich im Prinzip darin, was in `condition`, `S1` und `S2` stehen kann, wie diese Regeln geordnet sind (das längste `S1` gewinnt!), und ob es mehrere Durchläufe gibt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming mit regulären Ausdrücken\n",
    "\n",
    "Die einfachsten Stemming-Methoden verwenden reguläre Ausdrücke für Affixe, die getilgt werden, falls Sie am Ende der Wortform stehen. (Dann werden die Affixe auch *Suffixe* genannt.) Die Regeln sehen also so aus:\n",
    "\n",
    "- `(S1 steht am Ende) S1 -> \"\"`\n",
    "\n",
    "Mit der String-Methode `endswith()` lässt sich das ohne Probleme umsetzen (siehe https://www.nltk.org/book/ch03.html):  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(word):\n",
    "    for suffix in ['ing', 'ly', 'ed', 'ious', 'ies', 'ive', 'es', 's', 'ment']:\n",
    "         if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "    return word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dasselbe geht natürlich auch sehr einfach mit regulären Ausdrücken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def stem2(word):\n",
    "    regexp = r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)?$'\n",
    "    stem, suffix = re.findall(regexp, word)[0]\n",
    "    return stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['connection', 'i']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[stem2(word) for word in [\"connections\", \"is\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diese beiden einfachen Stemmer haben natürlich diverse Einschränkungen: \n",
    "- keine Mindestlänge\n",
    "- nur Tilgungen (Problem: *women* $\\Rightarrow$ *woman*)\n",
    "- keine Mehrfachtilgung (wie beim Beispiel mit *connections* oben)\n",
    "\n",
    "NLTK enthält ein Modul für die Entwicklung eines einfachen [RegExp-Stemmers](https://www.nltk.org/api/nltk.stem.html#module-nltk.stem.regexp) (`RegexpStemmer` in `nltk.stem`), der zumindest die Angabe einer Mindestlänge zulässt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['connection', 'is']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "st = RegexpStemmer('ing$|s$|e$|able$', min=4)\n",
    "[st.stem(word) for word in [\"connections\", \"is\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neben `RegexpStemmer` enthält NLTK aber auch fertige Stemmer für eine Reihe von Sprachen (darunter Arabisch!).\n",
    "\n",
    "## Stemmer fürs Englische\n",
    "\n",
    "\n",
    "Für das Englische stehen im Modul `nltk.stem` gleich drei Standard-Stemmer zu Auswahl (:\n",
    "\n",
    "- der Porter-Stemmer (`PorterStemmer`)\n",
    "    - Ein sehr früher Stemmer aus den späten 1970ern.\n",
    "    - https://tartarus.org/martin/PorterStemmer/index.html\n",
    "    - Regeln der Form `(condition) S1 -> S2` mit nur einem Durchlauf, aber unterschiedliche \"Schritte\".\n",
    "- der Lancaster-Stemmer (`LancasterStemmer`)\n",
    "    - rekursive Anwendung der Stemming-Regeln $\\Rightarrow$ Overstemming\n",
    "    - https://dl.acm.org/citation.cfm?id=101310\n",
    "- der Snowball-Stemmer (`SnowballStemmer`)\n",
    "    - Weiterentwicklung des Porter Stemmers\n",
    "    - derzeit der Standard-Stemmer\n",
    "    - verfügbar auch für andere Sprachen \n",
    "    - http://snowball.tartarus.org/texts/introduction.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "pst = PorterStemmer()\n",
    "lst = LancasterStemmer()\n",
    "sste = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Porter Stemmer      Lancaster Stemmer   Sowball Stemmer     \n",
      "connections         connect             connect             connect             \n",
      "connectionistic     connectionist       connect             connectionist       \n",
      "flies               fli                 fli                 fli                 \n",
      "friend              friend              friend              friend              \n",
      "friendship          friendship          friend              friendship          \n",
      "friends             friend              friend              friend              \n",
      "friendships         friendship          friend              friendship          \n",
      "stabil              stabil              stabl               stabil              \n",
      "destabilize         destabil            dest                destabil            \n",
      "misunderstandings   misunderstand       misunderstand       misunderstand       \n",
      "railroad            railroad            railroad            railroad            \n",
      "moonlight           moonlight           moonlight           moonlight           \n",
      "football            footbal             footbal             footbal             \n",
      "women               women               wom                 women               \n",
      "woman               woman               wom                 woman               \n",
      "ate                 ate                 at                  ate                 \n",
      "at                  at                  at                  at                  \n",
      "generously          gener               gen                 generous            \n"
     ]
    }
   ],
   "source": [
    "word_list = [\"connections\",\"connectionistic\",\"flies\",\"friend\", \"friendship\", \"friends\", \"friendships\",\"stabil\",\"destabilize\",\"misunderstandings\",\"railroad\",\"moonlight\",\"football\",\"women\",\"woman\",\"ate\",\"at\",\"generously\"]\n",
    "print(\"{0:20}{1:20}{2:20}{3:20}\".format(\"Word\",\"Porter Stemmer\",\"Lancaster Stemmer\",\"Sowball Stemmer\"))\n",
    "for word in word_list:\n",
    "    print(\"{0:20}{1:20}{2:20}{3:20}\".format(word,pst.stem(word),lst.stem(word),sste.stem(word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemmer fürs Deutsche\n",
    "\n",
    "\n",
    "NLTK enthält zwei fertige Stemmer für das Deutsche. \n",
    "\n",
    "- der Snowball Stemmer, den wir oben schon fürs Englische gesehen haben\n",
    "- der Cistem Stemmer\n",
    "    - entwickelt an der LMU München\n",
    "    - http://www.cis.lmu.de/~weissweiler/cistem/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem.cistem import Cistem\n",
    "sstg = SnowballStemmer(\"german\")\n",
    "cist = Cistem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Snowball Stemmer    Cistem Stemmer      \n",
      "Adler               adl                 adler               \n",
      "Adlers              adl                 adler               \n",
      "Adlern              adl                 adler               \n",
      "aß                  ass                 ass                 \n",
      "gab                 gab                 gab                 \n",
      "Häuser              haus                hau                 \n",
      "Hau                 hau                 hau                 \n",
      "Absurditäten        absurditat          absurditat          \n",
      "Mäuserich           mauserich           mauserich           \n"
     ]
    }
   ],
   "source": [
    "word_list = [\"Adler\",\"Adlers\",\"Adlern\",\"aß\",\"gab\",\"Häuser\",\"Hau\",\"Absurditäten\",\"Mäuserich\"]\n",
    "print(\"{0:20}{1:20}{2:20}\".format(\"Word\",\"Snowball Stemmer\",\"Cistem Stemmer\"))\n",
    "for word in word_list:\n",
    "    print(\"{0:20}{1:20}{2:20}\".format(word,sstg.stem(word),cist.stem(word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluierung\n",
    "\n",
    "Die Evaluierung eines Stemmers läuft im Prinzip so ab: \n",
    "\n",
    "- Es gibt eine festgelegte **Testmenge** und eine gewünschte Ausgabe (der sogenannte **Gold-Standard**).\n",
    "- Es wird mit bestimmten **Evaluationsmaßen** (Precision, Recall, F1-Measure, Accuracy) gemessen, wie nah der Stemmer für die Testmenge am Gold-Standard dran ist.\n",
    "- Es wird die **Geschwindigkeit** des Stemmers gemessen.\n",
    "\n",
    "Das grundsätzliche Problem: \n",
    "- Der Gold-Standard steht nicht wirklich fest und ist abhängig von der Anwendung und dem linguistischen Anspruch.\n",
    "- Es ist prinzipiell unklar, wie man Geschindigkeit und Qualität miteinander verrechnen kann.\n",
    "- Die Evaluationsergebnisse gelten nur für die Testmenge.\n",
    "\n",
    "Siehe das [Cistem-Papier](http://www.cis.lmu.de/~weissweiler/cistem/) für ein Beispiel für die Evaluation eines Stemmers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">Aufgaben II</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">A2:</span> Wenden Sie den Snowball-Stemmer auf die Textsorte \"news\" im Brown-Korpus an, indem die Wortform jeweils durch das Resultat des Stemmers ersetzt wird!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ConcatenatedCorpusView' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-268422941bcf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Snowball-Stemmer for the news category in brown dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbrown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategories\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'news'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msste\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnews\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mc:\\program files\\python36\\lib\\site-packages\\nltk\\stem\\snowball.py\u001b[0m in \u001b[0;36mstem\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m   1415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1416\u001b[0m         \"\"\"\n\u001b[0;32m-> 1417\u001b[0;31m         \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1419\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopwords\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ConcatenatedCorpusView' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "# Lösung zu A2\n",
    "# Snowball-Stemmer for the news category in brown dataset\n",
    "news = brown.words(categories=['news'])\n",
    "sste.stem(news)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">A3:</span> Was sind nun die 10 häufigsten \"Wortformen\" _ohne Stopwörter_ und um wieviel Prozent verringert sich die Anzahl der \"Wortformen\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lösung zu A3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stopwörter** sind eine geschlossene Klasse von Funktionswörter wie *der*, *und*, *sich* uvm.\n",
    "`stopwords` kann so wie `brown` mit der Funktion `words()` ausgelesen werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatisierung\n",
    "\n",
    "Die Lemmatisierung ist eine weitere Form der Wortnormalisierung und dem Stemming sehr ähnlich. \n",
    "\n",
    "Unter Lemmatisierung versteht man die Abbildung einer Wortform auf ein **Lemma**. \n",
    "\n",
    "*Definition Lemma* (siehe Folien)\n",
    "> Ein Lemma ist besteht aus allen Wortformen derselben Wortart, die aus einem Stamm mittels Flexion & Modifikation gebildet werden können.\n",
    "\n",
    "Was wir bei der Lemmatisierung aber eigentlich wollen ist nicht eine Menge von Wortformen, sondern **eine** Wortform, die das Lemma repräsentiert, die sogenannte *Basisform*, *Zitierform* oder *Lemmaform* -- kurz: Lemma.\n",
    "\n",
    "Es gibt bestimmte Konventionen, wie die Zitierform für ein Lemma gewählt wird. Z.B. ist üblicherweise das Lemma eines Verbs die Infinitivform und das Lemma eines Nomens die Nominativ-Singular-Form.  \n",
    "\n",
    "Wichtig ist aber eigentlich nur, dass unterschiedliche Lemmata unterschiedliche Ziterformen haben. \n",
    "\n",
    "- {*house*, *houses*, *housed*, *housing*} $\\Rightarrow$ <span style=\"font-variant: small-caps;\">house.V</span>\n",
    "- {*house*, *houses*} $\\Rightarrow$ <span style=\"font-variant: small-caps;\">house.N</span>\n",
    "\n",
    "So genau nehmen es aber Lemmatisierer nicht unbedingt und verlassen sich, was die Wortart betrifft, gerne auf den POS-Tagger. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordNet Lemmatizer\n",
    "\n",
    "WordNet ist ein elektronisches Lexikon mit semantischen Informationen in Form von Begriffsnetzen. Wir werden uns diese sehr einschlägige Ressource in einer der nächsten Sitzungen genauer anschauen. \n",
    "\n",
    "NLTK enthält einen Lemmatisierer auf Grundlage der morphologischen Informationen in WordNet. Daher muss WordNet zunächst mit `download()` heruntergeladen werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Einmal heruntergeladen kann der [WordNet-Lemmatisierer](https://www.nltk.org/api/nltk.stem.html#module-nltk.stem.wordnet) importiert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der WordNet-Lemmatisierer normalisiert eine Eingabe, indem in der WordNet-Datenbank das dazu passende Lemma gesucht und ausgegeben wird. Wird kein passendes Lemma gefunden, wird die Eingabe unverändert wieder ausgegeben.\n",
    "\n",
    "Unterschiede zum Snowball-Stemmer:\n",
    "- wesentlich langsamer, da dabei die WordNet-Datenbank durchsucht wird\n",
    "- Erkennung von **Vokalmodifikation** (*women* $\\Rightarrow$ *woman*) und **Unregelmäßigkeiten** (*mice* $\\Rightarrow$ *mouse*)\n",
    "- Ausgabe von vollständigen Wortformen\n",
    "- keine Normalisierung von unbekannten Wortformen (*flooked*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = [\"connections\",\"connectionistic\",\"better\",\"is\",\"flies\",\"friendly\",\"friendliest\",\"destabilize\",\"misunderstandings\",\"football\",\"mice\",\"women\",\"ate\",\"generously\",\"flooked\"]\n",
    "print(\"{0:20}{1:20}{2:20}\".format(\"Word\",\"Sowball Stemmer\",\"WordNet Lemmatizer\"))\n",
    "for word in word_list:\n",
    "    print(\"{0:20}{1:20}{2:20}\".format(word,sste.stem(word),wnl.lemmatize(word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardmäßig nimmt der WordNet-Lemmatisierer an, dass die Eingabe ein Nomen ist. Dies kann mit der zweiten Option geändert werden:\n",
    "\n",
    "- \"n\": Nomen\n",
    "- \"v\": Verb\n",
    "- \"a\": Adjektiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"is :\", wnl.lemmatize(\"is\",pos=\"v\"))\n",
    "print(\"better :\", wnl.lemmatize(\"better\",pos=\"a\"))\n",
    "print(\"mice :\", wnl.lemmatize(\"mice\",pos=\"a\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">Aufgaben III</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">A4:</span> Lemmatisieren Sie den News-Teil des Brown Corpus mit `wnl.lemmatize()`, indem Sie abhängig vom POS-Tag eines Worttokens das `pos`-Argument von `wnl.lemmatize()` spezifizieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lösung zu A4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hinweise:\n",
    "- Die POS-Tags des Brown Corpus mit Beschreibung finden Sie hier: https://en.wikipedia.org/wiki/Brown_Corpus#Part-of-speech_tags_used\n",
    "- Überlegen Sie sich, welche POS-Tags zu Adjektiven und Verben gehören ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalisierung von Nicht-Standard-Worten\n",
    "\n",
    "Für Worttoken wie Zahlen, Abkürzungen und Datumsangaben gibt es spezielle Normalisierungsregeln. Z.B. können Dezimalzahlen durch die Zahl 0 ersetzt werden, und Abkürzungen durch AAA. Der Vorteil dieser Normalisierung ist, dass der Wortschatz verringert und damit das statistische Sprachmodell für viele Aufgaben verbessert wird."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
