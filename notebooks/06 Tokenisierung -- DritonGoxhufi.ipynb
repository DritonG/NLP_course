{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Abgegeben von (Name, Vorname):</span> \n",
    "Goxhufi, Driton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Immer griffbereit:\n",
    "- Website: https://www.nltk.org/\n",
    "- Buch: https://www.nltk.org/book/\n",
    "- Module: https://www.nltk.org/py-modindex.html\n",
    "- Beispiele: http://www.nltk.org/howto/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sätze: Tokenisierung  \n",
    "\n",
    "Wörter treten gewöhnlich nicht isoliert auf, sondern gruppieren sich zu Chunks, Phrasen (oder Konsitutenten) und Sätzen. \n",
    "\n",
    "Bisher sind uns Worte und diese Gruppen im Brown Corpus fein säuberlich getrennt und voranalysiert begegnet, z.B. die Worte und Sätze im Brown Corpus:   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('The', 'AT'), ('Fulton', 'NP-TL'), ('County', 'NN-TL'), ('Grand', 'JJ-TL'), ('Jury', 'NN-TL'), ('said', 'VBD'), ('Friday', 'NR'), ('an', 'AT'), ('investigation', 'NN'), ('of', 'IN'), (\"Atlanta's\", 'NP$'), ('recent', 'JJ'), ('primary', 'NN'), ('election', 'NN'), ('produced', 'VBD'), ('``', '``'), ('no', 'AT'), ('evidence', 'NN'), (\"''\", \"''\"), ('that', 'CS'), ('any', 'DTI'), ('irregularities', 'NNS'), ('took', 'VBD'), ('place', 'NN'), ('.', '.')], [('The', 'AT'), ('jury', 'NN'), ('further', 'RBR'), ('said', 'VBD'), ('in', 'IN'), ('term-end', 'NN'), ('presentments', 'NNS'), ('that', 'CS'), ('the', 'AT'), ('City', 'NN-TL'), ('Executive', 'JJ-TL'), ('Committee', 'NN-TL'), (',', ','), ('which', 'WDT'), ('had', 'HVD'), ('over-all', 'JJ'), ('charge', 'NN'), ('of', 'IN'), ('the', 'AT'), ('election', 'NN'), (',', ','), ('``', '``'), ('deserves', 'VBZ'), ('the', 'AT'), ('praise', 'NN'), ('and', 'CC'), ('thanks', 'NNS'), ('of', 'IN'), ('the', 'AT'), ('City', 'NN-TL'), ('of', 'IN-TL'), ('Atlanta', 'NP-TL'), (\"''\", \"''\"), ('for', 'IN'), ('the', 'AT'), ('manner', 'NN'), ('in', 'IN'), ('which', 'WDT'), ('the', 'AT'), ('election', 'NN'), ('was', 'BEDZ'), ('conducted', 'VBN'), ('.', '.')], ...]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "\n",
    "brown.tagged_sents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun haben wir nicht immer das Glück, dass die Daten genau in dem Format vorliegen, das wir für die weitere Verarbeitung benötigen. Stattdessen kann es sein, dass wir zunächst Strings vor uns haben, die wir erst **tokenisieren** müssen:\n",
    "\n",
    "> On a \\\\$50,000 mortgage of 30 years at 8%, the monthly payment wouldn't be \\\\$366.88.\n",
    "\n",
    "> John Fitzgerald Kennedy wurde am 29. Mai 1917 als zweitältester Sohn von Joseph P. Kennedy und Rose Fitzgerald Kennedy in Brookline, Massachusetts geboren. Er stammte aus einer bedeutenden Familie: Sein Großvater mütterlicherseits war der demokratische Politiker John F. Fitzgerald. Seine jüngeren Brüder Robert – der 1968 ebenfalls einem Attentat zum Opfer fiel – und Edward spielten beide in der amerikanischen Geschichte des 20. Jahrhunderts als Politiker eine wesentliche Rolle.\n",
    "\n",
    "Zwei Arten der Tokenisierung sind hier für uns relevant: \n",
    "\n",
    "- **Wort-Tokenisierung:** Die Identifikation von Worten im laufenden Text -- dazu zählen auch Zahlen, Satzzeichen etc.\n",
    "- **Satz-Tokenisierung:** Die Identifikation von Sätzen im laufenden Text\n",
    "\n",
    "Daneben gibt es aber viele weitere Arten der Tokenisierung, z.B. nach Zeilen, Tabs, [Themen](https://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize.texttiling) usw.\n",
    "\n",
    "**Man beachte:** Obwohl die Tokenisierung gerne als trivialer Vorverarbeitungsschritt angesehen wird, können sich die üblichen Tokenisierer überraschend deutlich unterscheiden (siehe [Dridan & Oepen 2012](https://www.aclweb.org/anthology/P12-2074)). Außerdem ist die Tokenisierung bis zu einem bestimmten Grad sprachabhängig.  \n",
    "\n",
    "## Wort-Tokenisierung\n",
    "\n",
    "Ziel bei der Wort-Tokenisierung ist die Identifikation von Worten und Satzzeichen im laufenden Text. Für obiges Beispiel würden wir also gerne eine Liste dieser Elemente erhalten:\n",
    "\n",
    "    ['On', 'a', '$', '50,000', 'mortgage', ...]\n",
    "\n",
    "Als erstes kommt hier wahrscheinlich die Stringmethode `split()` in den Sinn, mit der sich ein String an den Leerzeichen auftrennen lässt: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['On', 'a', '$50,000', 'mortgage', 'of', '30', 'years', 'at', '8%,', 'the', 'monthly', 'payment', \"wouldn't\", 'be', '$366.88.']\n"
     ]
    }
   ],
   "source": [
    "s = \"On a $50,000 mortgage of 30 years at 8%, the monthly payment wouldn't be $366.88.\"\n",
    "print(s.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das ist schon mal eine ganz gute Annäherung. Satzzeichen und Symbole wie `$` und `%` werden aber nicht als eigenständige Elemente erkannt, weil hier keine Leerzeichen als Separatoren verwendet werden.\n",
    "\n",
    "Statt die Separatoren anzugeben, könnte man auch mit `re.findall()` spezifizieren, was überhaupt ein \"Wort\" sein soll:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['On', 'a', '$', '50,000', 'mortgage', 'of', '30', 'years', 'at', '8', '%', ',', 'the', 'monthly', 'payment', \"wouldn't\", 'be', '$', '366.88.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "s = \"On a $50,000 mortgage of 30 years at 8%, the monthly payment wouldn't be $366.88.\"\n",
    "\n",
    "print(re.findall(\"(\\$|\\%|[^\\%\\s]+)\",s)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Damit kommt man in unserem Beispiel bei den Symbolen `$` und `%` weiter, aber die Satzzeichen und Apostrophe bereiten immer noch Probleme, denn `,`, `'` und `.` können auch Bestandteile von Token sein. \n",
    "\n",
    "Die Lösung besteht darin, zuerst die fehlenden Separatoren einzufügen und erst dann die `split()`-Methode anzuwenden. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['On', 'a', '$', '50,000', 'mortgage', 'of', '30', 'years', 'at', '8', '%', ',', 'the', 'monthly', 'payment', 'would', \"n't\", 'be', '$', '366.88', '.']\n"
     ]
    }
   ],
   "source": [
    "s = \"On a $50,000 mortgage of 30 years at 8%, the monthly payment wouldn't be $366.88.\"\n",
    "\n",
    "s = re.sub('(\\S+), ', r'\\1 , ' , s)        # comma before whitespace\n",
    "s = re.sub('\\.$', ' .' , s)                # end period\n",
    "s = re.sub('(\\%|\\$)',r' \\1 ',s)            # percentage and Dollar symbol \n",
    "s = re.sub('(n\\'t)(\\.|\\,| )', r' \\1\\2',s)  # negation contraction\n",
    "\n",
    "print(s.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So funktioniert im Prinzip auch der NLTK-Tokenisierer, der mittels `word_tokenize(string, language='english')` aufgerufen werden kann, wobei natürlich ein paar Substitutionsregeln mehr zum Einsatz kommen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['On', 'a', '$', '50,000', 'mortgage', 'of', '30', 'years', 'at', '8', '%', ',', 'the', 'monthly', 'payment', 'would', \"n't\", 'be', '$', '366.88', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "s = \"On a $50,000 mortgage of 30 years at 8%, the monthly payment wouldn't be $366.88.\"\n",
    "print(word_tokenize(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Satz-Tokenisierung\n",
    "\n",
    "Die Wort-Tokenisierung (und insbesondere `word_tokenize()`) setzt in der Regel voraus, dass die Eingabestrings aus einzelnen Sätzen bestehen. Das sieht man daran, dass Punkte immer (und meistens auch nur dann) als Satzzeichen tokenisiert werden, wenn sie am Ende des Strings stehen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['John', 'Fitzgerald', 'Kennedy', 'wurde', 'am', '29.', 'Mai', '1917', 'als', 'zweitältester', 'Sohn', 'von', 'Joseph', 'P.', 'Kennedy', 'und', 'Rose', 'Fitzgerald', 'Kennedy', 'in', 'Brookline', ',', 'Massachusetts', 'geboren', '.']\n",
      "['John', 'Fitzgerald', 'Kennedy', 'wurde', 'am', '29.', 'Mai', '1917', 'als', 'zweitältester', 'Sohn', 'von', 'Joseph', 'P', '.']\n"
     ]
    }
   ],
   "source": [
    "s1 = \"John Fitzgerald Kennedy wurde am 29. Mai 1917 als zweitältester Sohn von Joseph P. Kennedy und Rose Fitzgerald Kennedy in Brookline, Massachusetts geboren.\" \n",
    "s2 = \"John Fitzgerald Kennedy wurde am 29. Mai 1917 als zweitältester Sohn von Joseph P.\"\n",
    "print(word_tokenize(s1, language='german'))\n",
    "print(word_tokenize(s2, language='german'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`word_tokenize()` erkennt im obigen Beispiel also nicht, dass `P.` eine Abkürzung ist. Die Unterscheidung zwischen Punkten in Abkürzungen und Punkten als Satzzeichen ist aber essentiell für die [Erkennung von Satzgrenzen](https://en.wikipedia.org/wiki/Sentence_boundary_disambiguation), d.h. für die **Satz-Tokenisierung**. \n",
    "\n",
    "Da Abkürzungen a priori keine festgelegte Form haben, liefern reguläre Ausdrücken alleine nicht immer zufriedenstellenden Ergebnisse. Stattdessen verwendet man üblicherweise überwachte und unüberwachte Lernverfahren, die über statistische Verteilungen in rohen oder schon analysierten Textdaten ermitteln, was eine Abkürzung ist und was nicht. Im Prinzip geht es dabei um Inferenzen wie: Tritt `P` statistisch signifikant mit `.` auf? Wenn ja, dann behandle `P.` als Abkürzung.  Man nennt diesen Ansatz deshalb auch **kollokationsbasiert**. \n",
    "\n",
    "Damit können über 90% der Satzgrenzen richtig erkannt werden [(Kiss & Strunk 2006)](https://www.aclweb.org/anthology/J06-4003/), wobei die tatsächliche Performanz stark von der Sprache, dem Test- und Trainingsset abhängt. NLTK enthält daher ein Werkzeug, um aus rohen Textdaten einen Satz-Tokenisierer zu lernen: \n",
    "\n",
    "     my_sent_tokenizer = PunktSentenceTokenizer(trainingData)\n",
    "     \n",
    "Dies ist insbesondere nützlich, wenn bestimmte Sprachen oder Textgenres analysiert werden sollen, für die es noch keinen angepassten Satz-Tokenisierer gibt.\n",
    "\n",
    "Der vorgefertigte Satz-Tokenisierer im NLTK wird mit `sent_tokenizer(string,language='english')` aufgerufen. Man beachte, dass die Spracheinstellung einen erheblichen Einfluss auf das Ergebnis haben kann:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['John Fitzgerald Kennedy wurde am 29.', 'Mai 1917 als zweitältester Sohn von Joseph P. Kennedy und Rose Fitzgerald Kennedy in Brookline, Massachusetts geboren.']\n",
      "['John Fitzgerald Kennedy wurde am 29. Mai 1917 als zweitältester Sohn von Joseph P. Kennedy und Rose Fitzgerald Kennedy in Brookline, Massachusetts geboren.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "s = \"John Fitzgerald Kennedy wurde am 29. Mai 1917 als zweitältester Sohn von Joseph P. Kennedy und Rose Fitzgerald Kennedy in Brookline, Massachusetts geboren.\" \n",
    "print(sent_tokenize(s,language='english'))\n",
    "print(sent_tokenize(s,language='german'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wenn wir auf diese Weise das ganze Eingangsbeispiel nach Sätzen tokenisieren, erhalten wir folgendes Ergebnis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['John Fitzgerald Kennedy wurde am 29. Mai 1917 als zweitältester Sohn von Joseph P. Kennedy und Rose Fitzgerald Kennedy in Brookline, Massachusetts geboren.', 'Er stammte aus einer bedeutenden Familie: Sein Großvater mütterlicherseits war der demokratische Politiker John F. Fitzgerald.', 'Seine jüngeren Brüder Robert – der 1968 ebenfalls einem Attentat zum Opfer fiel – und Edward spielten beide in der amerikanischen Geschichte des 20. Jahrhunderts als Politiker eine wesentliche Rolle.']\n"
     ]
    }
   ],
   "source": [
    "s = \"John Fitzgerald Kennedy wurde am 29. Mai 1917 als zweitältester Sohn von Joseph P. Kennedy und Rose Fitzgerald Kennedy in Brookline, Massachusetts geboren. Er stammte aus einer bedeutenden Familie: Sein Großvater mütterlicherseits war der demokratische Politiker John F. Fitzgerald. Seine jüngeren Brüder Robert – der 1968 ebenfalls einem Attentat zum Opfer fiel – und Edward spielten beide in der amerikanischen Geschichte des 20. Jahrhunderts als Politiker eine wesentliche Rolle.\" \n",
    "print(sent_tokenize(s,language='german'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es fällt auf, dass die Erkennung von Satzgrenzen bei `:` offensichtlich nicht so gut funktioniert wie beim Punkt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zurück zur Wort-Disambiguierung: Das Lesk-Verfahren\n",
    "\n",
    "Mit Wort- und Satz-Tokenisierern können wir nun ein weiteres \"wissensbasiertes\" Wort-Disambiguierungsverfahren implementieren, das auf [Michael Lesk](https://en.wikipedia.org/wiki/Mike_Lesk) zurückgeht. \n",
    "\n",
    "Die Idee des sogenannten **Lesk-Verfahrens** ist recht simpel: Seien $\\Sigma$ die Bedeutungen eines Wortes $w$ in einem Kontext $K$. Diejenige Bedeutung in $\\Sigma$, dessen Beschreibung (Glosse, Beispielverwendung) die größte Überschneidung mit $K$ hat, wird als Bedeutung von $w$ in $K$ ausgegeben. Kurz gesagt: $L(w,K) = \\arg\\max_{\\sigma_1,...,\\sigma_n} \\sigma_i \\cap K$\n",
    "\n",
    "**Ein Beispiel:** Das Wort *bank* hat im Englischen die Bedeutungen `bank.n.01` und `bank.n.02` mit den folgenden beiden Glossen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bank.n.01: sloping land (especially the slope beside a body of water)\n",
      "bank.n.02: a financial institution that accepts deposits and channels the money into lending activities\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "print('bank.n.01: ' + wn.synset('bank.n.01').definition())\n",
    "print('bank.n.02: ' + wn.synset('bank.n.02').definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wenn $K = $*The water washed away the sandy bank of the river*, dann würde der Lesk-Algorithmus die Bedeutung `bank.n.01` ausgeben, da die Überschneidung zwischen $K$ und `wn.synset('bank.n.01').definition()` abzüglich Stopwörter größer ist und *water* enthält.   \n",
    "\n",
    "Es gibt viele mögliche Varianten des Lesk-Algorithmus. Der Kontext $K$ kann hier z.B. bestehen aus: \n",
    "- den Wortformen der Worttoken in $K$\n",
    "- den möglichen Bedeutungen der Worttoken in $K$\n",
    "\n",
    "Die Bedeutungen in $\\Sigma$ können z.B. bestehen aus:\n",
    "- den Definitionen oder den Beispielen der Synsets, oder beidem\n",
    "- den Definitionen und Beispielen der durch Hyponymie direkt verbundenen Synsets (Banerjee & Pedersen 2002)\n",
    "\n",
    "\n",
    "### <span style=\"color:red\">Aufgaben I</span>\n",
    "\n",
    "Als nächstes wollen wir den Lesk-Algorithmus implementieren und für die Evaluation wieder die Senseval-Daten für *interest* verwenden. \n",
    "\n",
    "D.h. wir brauchen wieder das Dictionary `SV_SENSE_MAP`, um die Synset-Namen bei der Umwandlung der Senseval-Daten zu aktualisieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied from https://stackoverflow.com/a/16391584/6452961\n",
    "\n",
    "# A map of SENSEVAL senses to WordNet 3.0 senses.\n",
    "# SENSEVAL-2 uses WordNet 1.7, which is no longer installable on most modern\n",
    "# machines and is not the version that the NLTK comes with.\n",
    "# As a consequence, we have to manually map the following\n",
    "# senses to their equivalent(s).\n",
    "SV_SENSE_MAP = {\n",
    "    \"HARD1\": [\"difficult.a.01\"],    # not easy, requiring great physical or mental\n",
    "    \"HARD2\": [\"hard.a.02\",          # dispassionate\n",
    "              \"difficult.a.01\"],\n",
    "    \"HARD3\": [\"hard.a.03\"],         # resisting weight or pressure\n",
    "    \"interest_1\": [\"interest.n.01\"], # readiness to give attention\n",
    "    \"interest_2\": [\"interest.n.03\"], # quality of causing attention to be given to\n",
    "    \"interest_3\": [\"pastime.n.01\"],  # activity, etc. that one gives attention to\n",
    "    \"interest_4\": [\"sake.n.01\"],     # advantage, advancement or favor\n",
    "    \"interest_5\": [\"interest.n.05\"], # a share in a company or business\n",
    "    \"interest_6\": [\"interest.n.04\"], # money paid for the use of money\n",
    "    \"cord\": [\"line.n.18\"],          # something (as a cord or rope) that is long and thin and flexible\n",
    "    \"formation\": [\"line.n.01\",\"line.n.03\"], # a formation of people or things one beside another\n",
    "    \"text\": [\"line.n.05\"],                 # text consisting of a row of words written across a page or computer screen\n",
    "    \"phone\": [\"telephone_line.n.02\"],   # a telephone connection\n",
    "    \"product\": [\"line.n.22\"],       # a particular kind of product or merchandise\n",
    "    \"division\": [\"line.n.29\"],      # a conceptual separation or distinction\n",
    "    \"SERVE12\": [\"serve.v.02\"],       # do duty or hold offices; serve in a specific function\n",
    "    \"SERVE10\": [\"serve.v.06\"], # provide (usually but not necessarily food)\n",
    "    \"SERVE2\": [\"serve.v.01\"],       # serve a purpose, role, or function\n",
    "    \"SERVE6\": [\"service.v.01\"]      # be used by; as of a utility\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['interest.n.04', 18, [('yields', 'NNS'), ('on', 'IN'), ('money-market', 'JJ'), ('mutual', 'JJ'), ('funds', 'NNS'), ('continued', 'VBD'), ('to', 'TO'), ('slide', 'VB'), (',', ','), ('amid', 'IN'), ('signs', 'VBZ'), ('that', 'IN'), ('portfolio', 'NN'), ('managers', 'NNS'), ('expect', 'VBP'), ('further', 'JJ'), ('declines', 'NNS'), ('in', 'IN'), ('interest', 'NN'), ('rates', 'NNS'), ('.', '.')]]\n",
      "['', 18, [('yields', 'NNS'), ('on', 'IN'), ('money-market', 'JJ'), ('mutual', 'JJ'), ('funds', 'NNS'), ('continued', 'VBD'), ('to', 'TO'), ('slide', 'VB'), (',', ','), ('amid', 'IN'), ('signs', 'VBZ'), ('that', 'IN'), ('portfolio', 'NN'), ('managers', 'NNS'), ('expect', 'VBP'), ('further', 'JJ'), ('declines', 'NNS'), ('in', 'IN'), ('interest', 'NN'), ('rates', 'NNS'), ('.', '.')]]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import senseval\n",
    "interestGoldData = [[SV_SENSE_MAP[inst.senses[0]][0],inst.position,inst.context] \n",
    "                        for inst in senseval.instances('interest.pos')]\n",
    "interestTestData = [['', inst[1], inst[2]] for inst in interestGoldData]\n",
    "\n",
    "print(interestGoldData[0])\n",
    "print(interestTestData[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">A1:</span> Vervollständigen Sie die Funktion `lesk_disambiguate_interest` und implementieren Sie dabei eine Variante des Lesk-Algorithmus! Führen Sie für die Synset-Glossen und Synset-Beispielen die folgenden **Vorverarbeitungsschritte** aus: \n",
    "\n",
    "1. Wort- und Satz-Tokenisierung\n",
    "2. Entfernung von Stopwörtern (`from nltk.corpus import stopwords`), Satzzeichen etc.\n",
    "3. Lemmatisierung der übrigen Wortformen (`from nltk.stem import WordNetLemmatizer`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-104-e89dedcfee61>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-104-e89dedcfee61>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    synset_count = [[outsyns],[position],[context] for syns in interestGoldData]\u001b[0m\n\u001b[0m                                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "synset_count = [[outsyns],[position],[context] for syns in interestGoldData]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('interest.n.04')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset(interestGoldData[8][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(interestGoldData[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-d799474ac196>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mwn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minterestGoldData\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefinition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mwn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minterestGoldData\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefinition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msyns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msyns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefinition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msyns\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynsets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minterestGoldData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mc:\\program files\\python36\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py\u001b[0m in \u001b[0;36msynsets\u001b[0;34m(self, lemma, pos, lang, check_exceptions)\u001b[0m\n\u001b[1;32m   1573\u001b[0m         \u001b[0mof\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mlanguage\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1574\u001b[0m         \"\"\"\n\u001b[0;32m-> 1575\u001b[0;31m         \u001b[0mlemma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlemma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1577\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlang\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'eng'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "wn.synset(interestGoldData[0][0]).definition()\n",
    "wn.synset(interestGoldData[0][0]).definition()\n",
    "for i in range(len(interestGoldData[:][0])):\n",
    "    \n",
    "[[syns.name(),syns.definition()] for syns in wn.synset(interestGoldData)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a fixed charge for borrowing money; usually a percentage of the amount borrowed'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset(interestGoldData[0][0]).definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def lesk_disambiguate_interest(inst) :\n",
    "    outsyns = inst[0]\n",
    "    position = inst[1]\n",
    "    context = inst[2]   # [('yields', 'NNS'), ('on', 'IN'), ('money-market', 'JJ'), ...]\n",
    "    word_tokenize(s1, language='english')\n",
    "    # Lösung A1\n",
    " \n",
    "    \n",
    "    return outsyns   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test für A1\n",
    "\n",
    "sumTrueDisambiguations = 0\n",
    "sumFalseDisambiguations = 0\n",
    "\n",
    "for i in range(len(interestTestData)) :\n",
    "    if lesk_disambiguate_interest(interestTestData[i]) == interestGoldData[i][0] :\n",
    "        sumTrueDisambiguations += 1\n",
    "    else :\n",
    "        sumFalseDisambiguations += 1\n",
    "\n",
    "accuracyDisambiguations = sumTrueDisambiguations /(sumTrueDisambiguations + sumFalseDisambiguations)\n",
    "\n",
    "print(\"Accuracy: {}\".format(accuracyDisambiguations))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
