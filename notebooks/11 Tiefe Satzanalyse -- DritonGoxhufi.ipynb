{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Abgegeben von (Name, Vorname):</span> \n",
    "Musterfrau, Marina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Immer griffbereit:\n",
    "- Website: https://www.nltk.org/\n",
    "- Buch: https://www.nltk.org/book/ch08.html\n",
    "- Module: https://www.nltk.org/py-modindex.html\n",
    "- Beispiele: http://www.nltk.org/howto/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiefe Satzanalyse\n",
    "\n",
    "In den letzten Sitzungen haben wir Methoden kennengelernt, Chunks zu identifizieren und zu klassifzieren. Das Chunking ist aber nur eine sehr oberflächliche Form der Satzanalyse (deshalb **Shallow Parsing**), denn die Chunks sind ja nicht-rekursiv: \n",
    "\n",
    "    (S\n",
    "      (NP We/PRP)\n",
    "      see/VBD\n",
    "      (NP the/DT yellow/JJ dog/NN) \n",
    "      (PP of/IN)\n",
    "      (NP the/DT neighbor/NN))\n",
    "      \n",
    "Man kann also manche Beziehungen nicht ausdrücken, wie zum Beispiel hier die Zusammengehörigkeit von `the/DT yellow/JJ dog/NN of/IN the/DT neighbour/NN` als Objekt von `see` und gleichzeitig, dass `of/IN the/DT neighbour/NN` ein zusammenhängender Bestandteil dieses komplexen Objekts ist.\n",
    "\n",
    "Statt dieser flachen Struktur hätten wir also gerne eine \"tiefe\" NP:\n",
    "\n",
    "    (S\n",
    "      (NP We/PRP)\n",
    "      see/VBD\n",
    "      (NP the/DT yellow/JJ dog/NN \n",
    "         (PP of/IN\n",
    "            (NP the/DT neighbor/NN))))\n",
    "            \n",
    "Ein anderes sehr häufiges Beispiel für Rekursion betrifft in Sätze eingebettete Sätze:\n",
    "\n",
    "    (S\n",
    "       (NP Andre/NNP)\n",
    "       said/VBD\n",
    "      (S\n",
    "        (NP the/DT Jamaica/NNP Observer/NNP)\n",
    "        reported/VBD\n",
    "        (S \n",
    "           (NP Usain/NNP Bolt/NNP) \n",
    "           broke/VBD \n",
    "           (NP the/DT record/NN))))\n",
    "\n",
    "In diesem Notebook werden wir uns mit Verfahren beschäftigen, mit denen solche rekursiven, tiefen Strukturen erzeugt werden können."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaskadierte Chunker\n",
    "\n",
    "Eine recht simple Möglichkeit, rekursive Klammerstrukturen zu erzeugen, besteht darin, den Chunker auf bereits gechunkte Sätze anzuwenden. Erst dann entfalten rekursive Regeln wie `S:  {<NP><VB.*><NP|PP|S>+$}` ihre ganze Wirkung. \n",
    "\n",
    "Um zu erreichen, dass solche rekursive Regeln wiederholt angewendet werden, muss man bei der Instanziierung von `RegexpParser` den Parameter `loop` auf einen Wert $\\geq 2$ setzen:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = r\"\"\"\n",
    "  NP: {<DT|JJ|NN.*|PRP>+}          # Chunk sequences of DT, JJ, NN\n",
    "  PP: {<IN><NP>}               # Chunk prepositions followed by NP\n",
    "  S:  {<NP><VB.*><NP|PP|S>+$}  # Chunk verbs and their arguments\n",
    "  \"\"\"\n",
    "cp = nltk.RegexpParser(grammar,loop=3)\n",
    "\n",
    "sentence = [(\"I\",\"PRP\"), (\"think\",\"VBD\"),  (\"Andre\",\"NNP\"), (\"said\",\"VBD\"), (\"the\",\"DT\"), (\"Jamaica\",\"NNP\"), (\"Observer\", \"NNP\"), (\"reported\",\"VBD\"), (\"Usain\",\"NNP\"), (\"Bolt\",\"NNP\"), (\"broke\",\"VBD\"), (\"the\",\"DT\"), (\"record\", \"NN\")]\n",
    "\n",
    "print(cp.parse(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An diesem Beispiel sieht man aber auch sehr deutlich einen Nachteil von kaskadierten Chunkern (zumindest in der NLTK-Implementierung mit `RegexpParser`): Man muss die Anzahl der Wiederholungen und damit auch den maximalen Grad der Rekursion explizit vorgeben.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kontextfreie Grammatiken\n",
    "\n",
    "Im Unterschied zu kaskadierten Chunkern muss man bei [kontextfreien Grammatiken (CFG)](https://de.wikipedia.org/wiki/Kontextfreie_Grammatik) weder die Reihenfolge der Regelanwendung noch die Anzahl der Loops festlegen.   \n",
    "\n",
    "Für die Implementierung und Verarbeitung von CFGs stellt NLTK das Modul [`nltk.grammar.CFG`](https://www.nltk.org/api/nltk.html?highlight=cfg#nltk.grammar.CFG) bereit.\n",
    "\n",
    "CFGs bestehen im Wesentlichen aus Ersetzungsregeln der Form `A -> B C ...` und einem Startsymbol. Die Regeln können mit Hilfe der Methode `fromstring` eingegeben werden, wobei als Startsymbol `S` angenommen wird.\n",
    "\n",
    "Als Nichtterminale dienen üblicherweise die folgenden syntaktischen Phrasentypen (und deren \"Köpfe\"): \n",
    "\n",
    "| **Symbol** | **Meaning**             | **Example**          |\n",
    "| :----- | :------------------- | :--------------- |\n",
    "| S      | sentence             | *the man walked*   |\n",
    "| NP     | noun phrase          | *a dog*            |\n",
    "| VP     | verb phrase          | *saw a park*       |\n",
    "| PP     | prepositional phrase | *with a telescope* |\n",
    "| AP     | adjective phrase     | *very good*        |\n",
    "| Det    | determiner           | *the*              |\n",
    "| N      | noun                 | *dog*              |\n",
    "| V      | verb                 | *walked*           |\n",
    "| P      | preposition          | *in*               |\n",
    "| A      | adjective            | *yellow*           |\n",
    "\n",
    "Für den Satz *We see the yellow dog of the neighbour* könnten wir beispielsweise die folgende CFG angeben, wobei wir der Einfachheit halber die POS-Tags weglassen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_grammar = nltk.grammar.CFG.fromstring(\"\"\"\n",
    "S -> NP V NP\n",
    "NP -> Det A N PP | Det N | N\n",
    "PP -> P NP\n",
    "Det -> 'the' \n",
    "N -> 'We' | 'dog' | 'neighbour' \n",
    "A -> 'yellow'\n",
    "P -> 'of'\n",
    "V -> 'see'\n",
    "\"\"\")\n",
    "\n",
    "print(first_grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diese CFG können wir dann einem [`ChartParser`](https://de.wikipedia.org/wiki/Chart-Parser) übergeben, der damit den Satz *We saw the yellow dog of the neighbour* mit einer rekursiven NP analysiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_parser = nltk.ChartParser(first_grammar)\n",
    "\n",
    "sent = nltk.word_tokenize(\"We see the yellow dog of the neighbour\")\n",
    "\n",
    "for tree in first_parser.parse(sent):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Übrigens kann man mit `draw()` die Phrasenstruktur als Baum darstellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for tree in first_parser.parse(sent):\n",
    "#    tree.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die damit generierbaren Sätze lassen sich folgendermaßen erzeugen, wobei die Parameter `n` und `depth` möglichst niedrig gewählt werden sollten ;-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.parse.generate import generate\n",
    "\n",
    "for sent in generate(first_grammar,n=10,depth=4):\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ungrammatische Sätze\n",
    "\n",
    "Offensichtlich können wir mit `first_grammar` auch viele \"ungrammatische\" Sätze parsen/erzeugen, z.B. *the We see dog* oder *the dog see the dog*. Um dies zu vermeiden, müssen die Nichtterminale weiter differenziert und die Regeln angepasst werden. \n",
    "\n",
    "Man beachte, dass der Anspruch bei solchen CFG nicht nur darin besteht, eine vernünftige Analyse herauzubekommen, sondern auch genau die Menge der **grammatisch wohlgeformten Sätze** einer Sprache zu erzeugen.\n",
    "\n",
    "Um Sätze wie *the dog see the dog* zu verhindern und die Numeruskongruenz zwischen Subjekt und finitem Verb sicherzustellen, müsste man beispielsweise eine Unterscheidung wie `S -> NP_sg V_sg NP` und `S -> NP_pl V_pl NP` einführen.  \n",
    "\n",
    "Dies lässt sich eleganter mit [Merkmalsstrukturen](https://www.nltk.org/book/ch09.html) umsetzen, mit denen die beiden Regeln zusammengefasst werden könnten: `S -> NP[NUM=?n] V[NUM=?n] NP`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zusätzliche Rekursion\n",
    "\n",
    "Man beachte außerdem, dass wir bei der Regelspezifikation nicht mehr auf **reguläre Ausdrücke** zurückgreifen können wie noch beim Chunk-Parser. Es ist also nicht möglich, NP-Regel kompakt so anzugeben: `NP -> (Det)? (A)* N`. <span style=\"color:red\">(Frage am Rande: Warum ist das bei CFGs nicht zu empfehlen?)</span>\n",
    "\n",
    "Um an dieser Stelle eine beliebige Kette von Adjektiven zuzulassen, muss man weitere, zum Teil rekursive Regeln einführen, z.B. `NP -> Det N'` und `N' -> A N'` und `N' -> N`.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">Aufgaben I: Kontextfreie Grammatik</span>\n",
    "\n",
    "<span style=\"color:red\">A1:</span> Implementieren Sie eine kontextfreie Grammatik mit `nltk.grammar.CFG.fromstring`, die den Satz *I think Andre said the Jamaica Observer reported Usain Bolt broke the record* entsprechend der folgenden Struktur erzeugt: \n",
    "\n",
    "    (S\n",
    "      (NP I/PRP)\n",
    "      think/VBD\n",
    "      (S\n",
    "        (NP Andre/NNP)\n",
    "        said/VBD\n",
    "        (S\n",
    "          (NP the/DT Jamaica/NNP Observer/NNP)\n",
    "          reported/VBD\n",
    "          (S (NP Usain/NNP Bolt/NNP) broke/VBD (NP the/DT record/NN)))))\n",
    "          \n",
    "Nutzen Sie dabei die POS-Tags als Präterminale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lösung A1:\n",
    "\n",
    "bolt_grammar = nltk.grammar.CFG.fromstring(\"\"\"\n",
    "\n",
    "S -> Epsilon\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "bolt_parser = nltk.ChartParser(bolt_grammar)\n",
    "sent = nltk.word_tokenize(\"I think Andre said the Jamaica Observer reported Usain Bolt broke the record\")\n",
    "\n",
    "for tree in bolt_parser.parse(sent):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ambiguität\n",
    "\n",
    "Anders als beim Chunking mit dem `RegexpParser` sind beim CFG-Parsing oft mehrere Analysen für einen Satz möglich. Dieses Phänomen, dass sprachliche Zeichen mehrere Analysen erhalten können, nennt man in der Linguistik **Ambiguität**.\n",
    "\n",
    "Das NLTK-Buch zitiert ein klassisches Beispiel für **syntaktische oder strukturelle Ambiguität** (von Groucho Marx):\n",
    "\n",
    "> While hunting in Africa, I shot an elephant <span style=\"text-decoration:underline\">in my pajamas</span>.\n",
    "> How he got into my pajamas, I don't know.\n",
    "\n",
    "Der Witz entsteht durch die Ambiguität der PP *in my pajamay*, die sich sowohl auf *I* als auch auf *an elephant* beziehen kann. Man nennt das in der Literatur auch **PP-Attachment Ambiguity**. Nun ist die Kombination mit *I* sehr viel plausibler, wenn man das Weltwissen berücksichtigt. Aus dem Kontext wird aber klar, das eigentlich die (sehr viel unplausiblere) Kombination mit *an elephant* gemeint ist.\n",
    "\n",
    "An diesem Beispiel wird deutlich, dass Ambiguität an unvermuteten Stellen auftauchen kann. Außerdem kann auch die unplausibelste Bedeutung intendiert sein (und sei es nur zum Spaß). Eine Grammatik muss also prinzipiell alle Bedeutungen erzeugen/analysieren können. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im NLTK-Buch wird diese Ambiguität so modelliert, dass die PP *in my pajamay* entweder an der VP oder an der Objekt-NP \"angehängt\" wird:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groucho_grammar = nltk.grammar.CFG.fromstring(\"\"\"\n",
    "S -> NP VP\n",
    "PP -> P NP\n",
    "NP -> Det N | Det N PP | 'I'\n",
    "VP -> V NP | VP PP\n",
    "Det -> 'an' | 'my'\n",
    "N -> 'elephant' | 'pajamas'\n",
    "V -> 'shot'\n",
    "P -> 'in'\n",
    "\"\"\")\n",
    "\n",
    "parser = nltk.ChartParser(groucho_grammar)\n",
    "sent = ['I', 'shot', 'an', 'elephant', 'in', 'my', 'pajamas']\n",
    "\n",
    "for tree in parser.parse(sent):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for tree in list(parser.parse(sent)):\n",
    "#     tree.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die strukturelle Ambiguität (und natürlich auch die lexikalische Ambiguität) ist eine große Herausforderung für CFG-Parser (und für NLP-Verfahren im Allgemeinen). Durch die freie Kombinatorik der Möglichkeiten wächst der Suchraum abhängig von der Satzlänge oft sehr stark. Hier muss der Parser einen möglichst ressourcenschonenden Weg finden.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CFG-Parser\n",
    "\n",
    "NLTK enthält \"Apps\" für verschiedene Parsingansätze, mit denen man die Ansätze interaktiv erkunden kann.\n",
    "\n",
    "#### Recursive Descent Parser\n",
    "\n",
    "Ein einfacher [**(LL-)Top-Down-Parser**](https://de.wikipedia.org/wiki/LL-Parser), der ausgehend vom Startsymbol die Ausgabe von links nach rechts zu erzeugen versucht, ohne die Ausgabe bei der Wahl der Produktionen zu berücksichtigen. Bei der Verwendung linksrekursiver Regeln (`NP -> NP PP`) kommt es zu Endlosschleifen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.app.rdparser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shift-Reduce Parser\n",
    "\n",
    "Ein einfacher [**(LR-)Buttom-Up-Parser**](https://de.wikipedia.org/wiki/LR-Parser), der ausgehend von den Terminalen das Startsymbol zu erreichen versucht. Die Produktionen werden also quasi umgedreht, so dass die Symbole auf der rechten Seite (RHS) durch die Symbole auf der linken Seite (LHS) ersetzt werden. Endlosschleifen ergeben sich bei rekursiven unären Produktionen (`NP -> NP`). Durch den Verzicht aufs Backtracking benötigt der SR-Parser nur lineare Zeit, kann dafür aber auch nicht alle kontextfreie Sprachen erkennen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.app.srparser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chart Parsing\n",
    "\n",
    "Schließlich können diese (und andere) Parsingstrategien mit einem [Speichermechanismus](https://de.wikipedia.org/wiki/Chart-Parser) kombiniert werden, den man **Dynamic Programming** nennt. Das bedeutet im Grunde nichts anderes, als dass Zwischenergebnisse dauerhaft in einer Tabelle (\"Chart\") gespeichert und bei der Bearbeitung alternativer Parsingwege wiederverwendet werden können. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.app.chartparser_app.app()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependenzstrukturen\n",
    "\n",
    "Bisher haben wir uns mit sogenannten **Konstituenten** (aka Chunks oder Phrasen) beschäftigt, d.h. mit Gruppierungen von Worttoken, die Teil-Ganzes-Beziehungen widerspiegeln.\n",
    "\n",
    "In der **Dependenzgrammatik** wird dagegen nur mit Beziehungen (sogenannten **Dependenzen**) zwischen Worttoken gearbeitet; Konstituenten spielen dort allenfalls indirekt eine Rolle. Dies hat zur Folge, dass die syntaktischen Repräsentationen sparsamer sind, da es dort keine abstrakten Knoten gibt.  \n",
    "\n",
    "![](https://www.nltk.org/images/depgraph0.png)\n",
    "\n",
    "Die Dependenzen sind darüber hinaus funktional eindeutiger beschriftet. Beides hat gewisse Vorteile, so dass sich Dependenzstrukturen zu einem wichtigen Element in der NLP entwickelt haben. \n",
    "\n",
    "Auf der anderen Seite ist es oft nicht einfach, zu entscheiden, in welche Richtung die Dependenzen zeigen, d.h. welches Wort der Kopf ist und welches der Dependent. Hier lohnt ein Blick auf die Guidelines der [UD-Initiative](https://universaldependencies.org/guidelines.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK bietet leider nur eine rudimentäre Unterstützung von Dependenzstrukturen und Dependenzparsern. Wir können immerhin Dependenzgrammatiken wie CFGs aufschreiben (was kein Fehler ist) und einem Dependenzparser übergeben: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groucho_dep_grammar = nltk.DependencyGrammar.fromstring(\"\"\"\n",
    "'shot' -> 'I' | 'elephant' | 'in'\n",
    "'elephant' -> 'an' | 'in'\n",
    "'in' -> 'pajamas'\n",
    "'pajamas' -> 'my'\n",
    "\"\"\")\n",
    "\n",
    "print(groucho_dep_grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdp = nltk.ProjectiveDependencyParser(groucho_dep_grammar)\n",
    "sent = nltk.word_tokenize('I shot an elephant in my pajamas')\n",
    "trees = pdp.parse(sent)\n",
    "for tree in trees:\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leider sehe ich nicht, wie man hier noch Dependenzlabel hinzufügen kann."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastische/Gewichtete CFGs\n",
    "\n",
    "Zum Schluss noch der Hinweis, das angesichts der großen Anzahl der möglichen Parsinglösungen für einen Satz (Stichwort Ambiguität) nicht immer alle Lösungen ausgereichnet werden können. Zudem will man gemeinhin nicht 253 irgendwie mögliche Analysen erhalten, sondern die eine plausible. \n",
    "\n",
    "Um das zu erreichen, sind Verfahren zur Gewichtung von Analysen notwendig. Eine naheliegende Möglichkeit besteht darin, die Produktionen einer CFG mit Wahrscheinlichkeitsmaßen zu versehen (**Probabilistic CFG**). Dabei muss eigentlich nur darauf geachtet werden, dass die Wahrscheinlichkeiten der Produktionen mit derselben LHS in der Summe $1$ ergeben.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from NLTK\n",
    "grammar = nltk.PCFG.fromstring(\"\"\"\n",
    "    S    -> NP VP              [1.0]\n",
    "    VP   -> TV NP              [0.4]\n",
    "    VP   -> IV                 [0.3]\n",
    "    VP   -> DatV NP NP         [0.3]\n",
    "    TV   -> 'saw'              [1.0]\n",
    "    IV   -> 'ate'              [1.0]\n",
    "    DatV -> 'gave'             [1.0]\n",
    "    NP   -> 'telescopes'       [0.8]\n",
    "    NP   -> 'Jack'             [0.2]\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viterbi_parser = nltk.ViterbiParser(grammar)\n",
    "\n",
    "for tree in viterbi_parser.parse(['Jack', 'saw', 'telescopes']):\n",
    "    print(tree)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Wahrscheinlichkeitsmaße sowie die Grammatik werden üblicherweise direkt aus vorannotierten Corpora (sogenannte Baumbanken) induziert. Daneben gibt es aber auch große, handgefertigte Grammatiken. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
