{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Abgegeben von (Name, Vorname):</span> \n",
    "Goxhufi, Driton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordNet II: Distanz- und Ähnlichkeitsmaße \n",
    "\n",
    "Für dieses Notebook gibt es eine begeleitendes Skript auf Ilias: `skript-wordnet.pdf`\n",
    "\n",
    "Die Basiselemente von [WordNet](https://wordnet.princeton.edu) wurde in der letzten Sitzung bereits vorgestellt. In dieser Sitzung beschäftigen wir uns mit dem (indirekten) Verhältnis zwischen den Synsets in WordNet, wie wir damit die konzeptuelle Ähnlichkeit bemessen und schließlich ein Wort in einem Satz disambiguieren können.\n",
    "\n",
    "Wir importieren wie gewohnt das WordNet-Modul von NLTK. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Immer griffbereit:\n",
    "- Website: https://www.nltk.org/\n",
    "- Buch: https://www.nltk.org/book/\n",
    "- Module: https://www.nltk.org/py-modindex.html\n",
    "- Beispiele: http://www.nltk.org/howto/\n",
    "\n",
    "## Distanzmaße: Minimale Pfadlänge\n",
    "\n",
    "Die Synsets in WordNet sind über Relationskanten (Hyponomie, Hyperonymie, Meronymie, Antonymie, ...) verbunden, die als Sequenz einen sogenannten **Pfad** bilden. Da wir es hier mit einem Graphen zu tun haben, kann es mehrere solcher Pfade geben (im Unterschied zu Bäumen). \n",
    "\n",
    "<img src=\"Fellbaum(2006)-Figure1-part.png\" alt=\"aus Fellbaum (2006)\" style=\"width: 500px;\"/>\n",
    "\n",
    "Am interessantesten ist sicherlich der kürzeste Pfad zwischen zwei Synsets: \n",
    "\n",
    "$$pathlen(s_1 ,s_2 ) = \\text{die Anzahl der Kanten im kürzesten Pfad zwischen } s_1 \\text{ und }\n",
    "s_2$$\n",
    "\n",
    "NLTK stellt hier die Methode `shortest_path_distance()` für Synset-Objekte zur Verfügung. Aber **Vorsicht**: `shortest_path_distance()` betrachtet nur die Hyponymie-Relation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ergebnisse von shortest_path_distance() bei unterschiedlichen Relationstypen:\n",
      "gemeinsames direktes Hyponym: 5\n",
      "Hyponymie: 1\n",
      "Meronymie: 4\n",
      "Antonymie: 2\n",
      "Derivationally related: None\n"
     ]
    }
   ],
   "source": [
    "print(\"Ergebnisse von shortest_path_distance() bei unterschiedlichen Relationstypen:\")\n",
    "print(\"gemeinsames direktes Hyponym: {}\".format(wn.synset('organism.n.01').shortest_path_distance(wn.synset('causal_agent.n.01'))))\n",
    "print(\"Hyponymie: {}\".format(wn.synset('bank.n.01').shortest_path_distance(wn.synset('waterside.n.01'))))\n",
    "print(\"Meronymie: {}\".format(wn.synset('water.n.01').shortest_path_distance(wn.synset('oxygen.n.01'))))\n",
    "print(\"Antonymie: {}\".format(wn.synset('living.n.02').shortest_path_distance(wn.synset('dead.n.01'))))\n",
    "print(\"Derivationally related: {}\".format(wn.synset('decision.n.01').shortest_path_distance(wn.synset('decide.v.01'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('organism.n.01').shortest_path_distance(wn.synset('causal_agent.n.01'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">Aufgaben I</span>\n",
    "\n",
    "<span style=\"color:red\">A1:</span> Schreiben Sie eine Funktion `shortest_hyponomy_pathlength_of_lemmas(lemma1,lemma2)`, die für zwei **Lemmaformen** `lemma1` und `lemma2` den kürzesten Hyponomie-Pfad als ganze Zahl und die entsprechenden Synset-Namen ausgibt! Verwenden Sie als Ausgabeformat eine Liste mit drei Elementen:\n",
    "\n",
    "    shortest_hyponomy_pathlength_of_lemmas('bank','waterside') -->  [1,'bank.n.01','waterside.n.01']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma('bank.n.01.bank')\n",
      "Lemma('waterside.n.01.waterside')\n"
     ]
    }
   ],
   "source": [
    "l_bank = wn.lemmas('bank')[0]\n",
    "l_waterside = wn.lemmas('waterside')[0]\n",
    "print(l_bank)\n",
    "print(l_waterside)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('slope.n.01'),\n",
       " Synset('geological_formation.n.01'),\n",
       " Synset('object.n.01'),\n",
       " Synset('physical_entity.n.01'),\n",
       " Synset('entity.n.01')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyp = lambda s:s.hypernyms()\n",
    "list(l_bank.synset().closure(hyp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_bank.synset().shortest_path_distance(l_waterside.synset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lösung A1\n",
    "def shortest_hyponomy_pathlength_of_lemmas(lemma1, lemma2):\n",
    "    shpl = []\n",
    "    \n",
    "    l1 = wn.lemmas(lemma1)[0]\n",
    "    l2 = wn.lemmas(lemma2)[0]\n",
    "    l1_synset = l1.synset()\n",
    "    l2_synset = l2.synset()\n",
    "    \n",
    "    d =l1_synset.shortest_path_distance(l2_synset)\n",
    "    shpl.append(d)\n",
    "    shpl.append(l1_synset.name())\n",
    "    shpl.append(l2_synset.name())\n",
    "    \n",
    "    return shpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"bank      , waterside : [1, 'bank.n.01', 'waterside.n.01']\",\n",
       " \"person    , cause     : [10, 'person.n.01', 'cause.n.01']\",\n",
       " \"person    , organism  : [1, 'person.n.01', 'organism.n.01']\",\n",
       " \"cause     , organism  : [12, 'cause.n.01', 'organism.n.01']\"]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test für A1\n",
    "\n",
    "[\"{:10}, {:10}: {}\".format(lemma1,lemma2,shortest_hyponomy_pathlength_of_lemmas(lemma1,lemma2))\n",
    "    for  lemma1,lemma2 \n",
    "    in [['bank','waterside'],\n",
    "        ['person','cause'],\n",
    "        ['person','organism'],\n",
    "        ['cause','organism']]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ähnlichkeitsmaße\n",
    "\n",
    "Es gibt eine Reihe von Größen, die die semantische Ähnlichkeit oder semantische Nähe von zwei Synsets bemessen soll. \n",
    "\n",
    "### Pfadbasiert\n",
    "\n",
    "Die Pfadlänge an sich ist ein wichtiger und einfacher Indikator für die semantische Ähnlichkeit oder semantische Nähe von zwei Synsets: Je kleiner die (kleinste) Pfadlänge, desto ähnlicher sind die Synsets und die enthaltenen Lexeme/Lemmata.\n",
    "\n",
    "$$sim_{path}(s_1,s_2) = \\frac{1}{pathlen(s_1,s_2)+1}$$\n",
    "\n",
    "NLTK stellt dafür die Methode `path_similarity()` zur Verfügung.\n",
    "\n",
    "### Pfadbasiert + LCS\n",
    "\n",
    "Eine Variante der pfadbasierten Ähnlichkeit, die die Tiefe der Synsets berücksichtigt ist die **Wu-Palmer Similarity** (`wup-similarity()`). Die Idee ist hier, dass sich die Einbettungstiefe $depth$ des \"lowest common subsumer\" $lcs$ (niedrigstes gemeinsames Hypernym) positiv auf das Ähnlichkeitsmaß auswirkt. \n",
    "\n",
    "$$sim_{wup}(s_1,s_2) = \\frac{2 * depth(lcs(s_1,s_2))}{pathlen(s_1,lcs(s_1,s_2))+pathlen(s_2,lcs(s_1,s_2))}$$\n",
    "\n",
    "### Wahrscheinlichkeiten der Synsets und des LCS\n",
    "\n",
    "Es gibt aber auch Ähnlichkeitsmaße, die Pfade außer Acht lassen und nur die Wahrscheinlichkeit der Synsets und deren LCS betrachten. Die benötigten Wahrscheinlichkeiten können z.B. mit Wort-Frequenzen in Corpora abgeschätzt werden:\n",
    "\n",
    "$$P(s) = \\frac{\\sum_{w \\in lemmas(s)} count(w)}{N}$$\n",
    "\n",
    "Die Wahrscheinlichkeit der Synsets nimmt damit tendenziell ab, je spezifischer sie sind: \n",
    "\n",
    "<img src=\"Jurafsky,Martin(2018)-FigureC.6.PNG\" alt=\"aus Jurafsky & Martin (2018)\" style=\"width: 220px;\"/>\n",
    "\n",
    "Ein bekanntes Ähnlichkeitsmaß ist hier die **Resnik Similarity** (`res-similarity()`), die nur die Wahrscheinlichkeit (oder genauer gesagt den Informationsgehalt) des LCS betrachtet:\n",
    "\n",
    "$$sim_{Resnik}(s_1,s_2) = -\\log P(lcs(s_1,s_2))$$\n",
    "\n",
    "Weitentwicklungen der Resnik Similarity berücksichtigen auch die Wahrscheinlichkeit der Synsets (und damit indirekt die Pfadlänge zum LCS).\n",
    "\n",
    "NLTK enthält davon:\n",
    "- **Lin Similarity** (`lin-similarity()`): $sim_{Lin}(s_1,s_2) = \\frac{2 * \\log P(lcs(s_1,s_2))}{\\log P(s_1) + P(s_2)}$\n",
    "- **Jiang-Conrath Distance** (`jcn_similarity()`): $sim_{JC}(s_1,s_2) = 2 * \\log P(lcs(s_1,s_2)) - (\\log P(s_1) + P(s_2))$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synsets: bank.n.01, sea.n.01\n",
      "path_sim: 0.125\n",
      "wup_sim: 0.36363636363636365\n",
      "res_sim: 0.8017591149538994\n",
      "lin_sim: 0.0837782004484866\n",
      "jcn_sim: 0.05702384600192556\n",
      "\n",
      "Synsets: bank.n.01, depository_financial_institution.n.01\n",
      "path_sim: 0.07692307692307693\n",
      "wup_sim: 0.14285714285714285\n",
      "res_sim: -0.0\n",
      "lin_sim: -0.0\n",
      "jcn_sim: 0.05165835201605558\n",
      "\n",
      "Synsets: organism.n.01, causal_agent.n.01\n",
      "path_sim: 0.16666666666666666\n",
      "wup_sim: 0.4444444444444444\n",
      "res_sim: 0.8017591149538994\n",
      "lin_sim: 0.3557998922959274\n",
      "jcn_sim: 0.3444380465758159\n"
     ]
    }
   ],
   "source": [
    "# Die wahrscheinlichkeitsbezogenen Maße benötigen zusätzliche Daten, \n",
    "# die zuvor importiert werden müssen.  \n",
    "from nltk.corpus import wordnet_ic\n",
    "brown_ic = wordnet_ic.ic('ic-brown.dat')\n",
    "semcor_ic = wordnet_ic.ic('ic-semcor.dat')\n",
    "\n",
    "def compare_similarity_measures(s1,s2) :\n",
    "    print(\"Synsets: {}, {}\".format(s1.name(),s2.name()))\n",
    "    print(\"path_sim: {}\".format(s1.path_similarity(s2)))\n",
    "    print(\"wup_sim: {}\".format(s1.wup_similarity(s2)))\n",
    "    print(\"res_sim: {}\".format(s1.res_similarity(s2,brown_ic)))\n",
    "    print(\"lin_sim: {}\".format(s1.lin_similarity(s2,brown_ic)))\n",
    "    print(\"jcn_sim: {}\".format(s1.jcn_similarity(s2,brown_ic)))\n",
    "\n",
    "compare_similarity_measures(wn.synset('bank.n.1'),wn.synset('sea.n.1'))\n",
    "print(\"\")\n",
    "compare_similarity_measures(wn.synset('bank.n.1'),wn.synset('bank.n.2'))\n",
    "print(\"\")\n",
    "compare_similarity_measures(wn.synset('organism.n.01'),wn.synset('causal_agent.n.01'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">Aufgaben II</span>\n",
    "\n",
    "Wir können nun Aufgabe A1 so erweitern, dass $lemmasim$ für zwei Lemmaformen ($l_1, l_2$) und eines der genannten Ähnlichkeitsmaße ($sim$) berechnet wird.\n",
    "\n",
    "$$lemmasim(l_1,l_2) = \\max_{s_1 \\in synsets(l_1)\\\\s_2 \\in synsets(l_2)} sim(s_1,s_2)$$\n",
    "\n",
    "<span style=\"color:red\">A2:</span> Schreiben Sie eine Funktion `lemmasim(l1,l2,sim)`, die bezogen auf ein Ähnlichkeitsmaß `sim` (path, wup, res, lin, jcn) den höchsten Wert und die dazugehörigen Synsets ausgibt!  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lösung A2\n",
    "def lemmasim(l1, l2, sim):\n",
    "    shlp = []\n",
    "    best = 0\n",
    "    \n",
    "    l1 = wn.lemmas(l1)[0]\n",
    "    l2 = wn.lemmas(l2)[0]\n",
    "    l1_synset = l1.synset()\n",
    "    l2_synset = l2.synset()\n",
    "    \n",
    "    # s1.name(),s2.name()\n",
    "    if sim == \"path\":\n",
    "        best = l1_synset.path_similarity(l2_synset)\n",
    "    elif sim == \"wup\":\n",
    "        best = l1_synset.wup_similarity(l2_synset)\n",
    "    elif sim == \"res\":\n",
    "        best = l1_synset.res_similarity(l2_synset,brown_ic)\n",
    "    elif sim == \"lin\":\n",
    "        best = l1_synset.lin_similarity(l2_synset,brown_ic)\n",
    "    elif sim == \"jcn\":\n",
    "        best = l1_synset.jcn_similarity(l2_synset,brown_ic)\n",
    "    else:\n",
    "        print(\"Your input was not acceped. Please use one of these similarity measure: path, wup, res, lin, jcn\")\n",
    "    shlp.append(best)\n",
    "    shlp.append(l1_synset.name())\n",
    "    shlp.append(l2_synset.name())\n",
    "    \n",
    "    return shlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9230769230769231"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_bank.synset().wup_similarity(l_waterside.synset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"bank      , waterside : [0.9230769230769231, 'bank.n.01', 'waterside.n.01']\",\n",
       " \"person    , cause     : [0.16666666666666666, 'person.n.01', 'cause.n.01']\",\n",
       " \"person    , organism  : [0.9230769230769231, 'person.n.01', 'organism.n.01']\",\n",
       " \"cause     , organism  : [0.14285714285714285, 'cause.n.01', 'organism.n.01']\"]"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tests für A2\n",
    "\n",
    "[\"{:10}, {:10}: {}\".format(lemma1,lemma2,lemmasim(lemma1,lemma2,\"wup\"))\n",
    "    for  lemma1,lemma2 \n",
    "    in [['bank','waterside'],\n",
    "        ['person','cause'],\n",
    "        ['person','organism'],\n",
    "        ['cause','organism']]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exkursion: SemCor\n",
    "\n",
    "SemCor ist ein mit WordNet-Synsets annotierter Teil des Brown-Corpus (u.a.) und umfasst ca. 240 000 Worttoken.\n",
    "\n",
    "SemCor ist in NLTK enthalten und als `corpus.reader`-Modul abrufbar: https://www.nltk.org/api/nltk.corpus.reader.html#module-nltk.corpus.reader.semcor \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(DT The)',\n",
       " \"(Lemma('group.n.01.group') (NE (NNP Fulton County Grand Jury)))\",\n",
       " \"(Lemma('state.v.01.say') (VB said))\"]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import semcor\n",
    "\n",
    "list(map(str, semcor.tagged_chunks(tag='both')[:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anwendung: Disambiguierung von *interest*\n",
    "\n",
    "Wie können wir nun mit den oben behandelten Ähnlichkeitsmaßen ein Token disambiguieren? Und wie unterscheiden sich die Ähnlichkeitsmaße in ihrer \"Treffsicherheit\"?\n",
    "\n",
    "Wir werden uns hier auf ein Nomen konzentrieren, nämlich *interest* mit seinen 7 Bedeutungen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['interest.n.01',\n",
       "  'a sense of concern with and curiosity about someone or something'],\n",
       " ['sake.n.01', 'a reason for wanting something done'],\n",
       " ['interest.n.03',\n",
       "  \"the power of attracting or holding one's attention (because it is unusual or exciting etc.)\"],\n",
       " ['interest.n.04',\n",
       "  'a fixed charge for borrowing money; usually a percentage of the amount borrowed'],\n",
       " ['interest.n.05',\n",
       "  '(law) a right or legal share of something; a financial involvement with something'],\n",
       " ['interest.n.06',\n",
       "  '(usually plural) a social group whose members control some field of activity and who have common aims'],\n",
       " ['pastime.n.01',\n",
       "  \"a diversion that occupies one's time and thoughts (usually pleasantly)\"]]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[syns.name(),syns.definition()] for syns in wn.synsets('interest',pos='n')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warum *interest*? Weil dafür Testdaten aus einem [Senseval-Wettbewerb](https://en.wikipedia.org/wiki/SemEval) in NLTK vorliegen.\n",
    "\n",
    "### Exkurs: Vorbereitung Senseval-Testdaten\n",
    "\n",
    "Die Senseval-Testdaten sind Teil von NLTK (https://www.nltk.org/api/nltk.corpus.reader.html#module-nltk.corpus.reader.senseval), müssen aber zunächst vorbereitet werden. \n",
    "\n",
    "Es gibt unterschiedliche \"Dateien\" für unterschiedliche Lemmaformen, die bei dem Senseval-Wettbewerb disambiguiert werden sollten (\"target words\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hard.pos', 'interest.pos', 'line.pos', 'serve.pos']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import senseval\n",
    "senseval.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Davon interessiert uns aber nur `interest.pos`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SensevalInstance(word='interest-n', position=18, context=[('yields', 'NNS'), ('on', 'IN'), ('money-market', 'JJ'), ('mutual', 'JJ'), ('funds', 'NNS'), ('continued', 'VBD'), ('to', 'TO'), ('slide', 'VB'), (',', ','), ('amid', 'IN'), ('signs', 'VBZ'), ('that', 'IN'), ('portfolio', 'NN'), ('managers', 'NNS'), ('expect', 'VBP'), ('further', 'JJ'), ('declines', 'NNS'), ('in', 'IN'), ('interest', 'NN'), ('rates', 'NNS'), ('.', '.')], senses=('interest_6',)), SensevalInstance(word='interest-n', position=7, context=[('longer', 'RB'), ('maturities', 'NNS'), ('are', 'VBP'), ('thought', 'VBN'), ('to', 'TO'), ('indicate', 'VB'), ('declining', 'VBG'), ('interest', 'NN'), ('rates', 'NNS'), ('because', 'IN'), ('they', 'PRP'), ('permit', 'VBP'), ('portfolio', 'NN'), ('managers', 'NNS'), ('to', 'TO'), ('retain', 'VB'), ('relatively', 'RB'), ('higher', 'JJR'), ('rates', 'NNS'), ('for', 'IN'), ('a', 'DT'), ('longer', 'RB'), ('period', 'NN'), ('.', '.')], senses=('interest_6',)), ...]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import senseval\n",
    "senseval.instances('interest.pos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Attribute dieser Instanzen können wir ganz leicht einzeln ausgeben: `senseval.instances('interest.pos')[0].word` etc.\n",
    "\n",
    "Wir stellen aber fest, dass die Synset-Namen nicht stimmen (z.B. `interest_6`). Es handelt sich dabei um alte WordNet-Bezeichnungen, die wir mit `SV_SENSE_MAP` übersetzen werden: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied from https://stackoverflow.com/a/16391584/6452961\n",
    "\n",
    "# A map of SENSEVAL senses to WordNet 3.0 senses.\n",
    "# SENSEVAL-2 uses WordNet 1.7, which is no longer installable on most modern\n",
    "# machines and is not the version that the NLTK comes with.\n",
    "# As a consequence, we have to manually map the following\n",
    "# senses to their equivalent(s).\n",
    "SV_SENSE_MAP = {\n",
    "    \"HARD1\": [\"difficult.a.01\"],    # not easy, requiring great physical or mental\n",
    "    \"HARD2\": [\"hard.a.02\",          # dispassionate\n",
    "              \"difficult.a.01\"],\n",
    "    \"HARD3\": [\"hard.a.03\"],         # resisting weight or pressure\n",
    "    \"interest_1\": [\"interest.n.01\"], # readiness to give attention\n",
    "    \"interest_2\": [\"interest.n.03\"], # quality of causing attention to be given to\n",
    "    \"interest_3\": [\"pastime.n.01\"],  # activity, etc. that one gives attention to\n",
    "    \"interest_4\": [\"sake.n.01\"],     # advantage, advancement or favor\n",
    "    \"interest_5\": [\"interest.n.05\"], # a share in a company or business\n",
    "    \"interest_6\": [\"interest.n.04\"], # money paid for the use of money\n",
    "    \"cord\": [\"line.n.18\"],          # something (as a cord or rope) that is long and thin and flexible\n",
    "    \"formation\": [\"line.n.01\",\"line.n.03\"], # a formation of people or things one beside another\n",
    "    \"text\": [\"line.n.05\"],                 # text consisting of a row of words written across a page or computer screen\n",
    "    \"phone\": [\"telephone_line.n.02\"],   # a telephone connection\n",
    "    \"product\": [\"line.n.22\"],       # a particular kind of product or merchandise\n",
    "    \"division\": [\"line.n.29\"],      # a conceptual separation or distinction\n",
    "    \"SERVE12\": [\"serve.v.02\"],       # do duty or hold offices; serve in a specific function\n",
    "    \"SERVE10\": [\"serve.v.06\"], # provide (usually but not necessarily food)\n",
    "    \"SERVE2\": [\"serve.v.01\"],       # serve a purpose, role, or function\n",
    "    \"SERVE6\": [\"service.v.01\"]      # be used by; as of a utility\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt können wir die Umwandlung der Senseval-Daten für *interest* vornehmen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['interest.n.04', 18, [('yields', 'NNS'), ('on', 'IN'), ('money-market', 'JJ'), ('mutual', 'JJ'), ('funds', 'NNS'), ('continued', 'VBD'), ('to', 'TO'), ('slide', 'VB'), (',', ','), ('amid', 'IN'), ('signs', 'VBZ'), ('that', 'IN'), ('portfolio', 'NN'), ('managers', 'NNS'), ('expect', 'VBP'), ('further', 'JJ'), ('declines', 'NNS'), ('in', 'IN'), ('interest', 'NN'), ('rates', 'NNS'), ('.', '.')]]\n",
      "['', 18, [('yields', 'NNS'), ('on', 'IN'), ('money-market', 'JJ'), ('mutual', 'JJ'), ('funds', 'NNS'), ('continued', 'VBD'), ('to', 'TO'), ('slide', 'VB'), (',', ','), ('amid', 'IN'), ('signs', 'VBZ'), ('that', 'IN'), ('portfolio', 'NN'), ('managers', 'NNS'), ('expect', 'VBP'), ('further', 'JJ'), ('declines', 'NNS'), ('in', 'IN'), ('interest', 'NN'), ('rates', 'NNS'), ('.', '.')]]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import senseval\n",
    "interestGoldData = [[SV_SENSE_MAP[inst.senses[0]][0],inst.position,inst.context] \n",
    "                        for inst in senseval.instances('interest.pos')]\n",
    "interestTestData = [['', inst[1], inst[2]] for inst in interestGoldData]\n",
    "\n",
    "print(interestGoldData[0])\n",
    "print(interestTestData[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">Aufgaben III</span>\n",
    "\n",
    "Mit den Testdaten in `interestTestData` und den Golddaten in `interestGoldData` können wir verschiedene Ansätze und Ähnlichkeitsmaße für die Disambiguierung von *interest* evaluieren. \n",
    "\n",
    "<span style=\"color:red\">A3:</span> Vervollständigen Sie die Funktion `disambiguate_interest`, indem Sie mindestens eine der oben erwähnten Ähnlichkeitsmetriken verwenden!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 18,\n",
       " [('yields', 'NNS'),\n",
       "  ('on', 'IN'),\n",
       "  ('money-market', 'JJ'),\n",
       "  ('mutual', 'JJ'),\n",
       "  ('funds', 'NNS'),\n",
       "  ('continued', 'VBD'),\n",
       "  ('to', 'TO'),\n",
       "  ('slide', 'VB'),\n",
       "  (',', ','),\n",
       "  ('amid', 'IN'),\n",
       "  ('signs', 'VBZ'),\n",
       "  ('that', 'IN'),\n",
       "  ('portfolio', 'NN'),\n",
       "  ('managers', 'NNS'),\n",
       "  ('expect', 'VBP'),\n",
       "  ('further', 'JJ'),\n",
       "  ('declines', 'NNS'),\n",
       "  ('in', 'IN'),\n",
       "  ('interest', 'NN'),\n",
       "  ('rates', 'NNS'),\n",
       "  ('.', '.')]]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = interestTestData[0]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interestTestData[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'interest.n.04'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interestGoldData[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('interest_6',)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senseval.instances('interest.pos')[0].senses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 18 [('yields', 'NNS'), ('on', 'IN'), ('money-market', 'JJ'), ('mutual', 'JJ'), ('funds', 'NNS'), ('continued', 'VBD'), ('to', 'TO'), ('slide', 'VB'), (',', ','), ('amid', 'IN'), ('signs', 'VBZ'), ('that', 'IN'), ('portfolio', 'NN'), ('managers', 'NNS'), ('expect', 'VBP'), ('further', 'JJ'), ('declines', 'NNS'), ('in', 'IN'), ('interest', 'NN'), ('rates', 'NNS'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "o = interestTestData[0][0]\n",
    "p = interestTestData[0][1]\n",
    "c = interestTestData[0][2]\n",
    "print(o,p,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('yields', 'NNS'),\n",
       " ('on', 'IN'),\n",
       " ('money-market', 'JJ'),\n",
       " ('mutual', 'JJ'),\n",
       " ('funds', 'NNS'),\n",
       " ('continued', 'VBD'),\n",
       " ('to', 'TO'),\n",
       " ('slide', 'VB'),\n",
       " (',', ','),\n",
       " ('amid', 'IN'),\n",
       " ('signs', 'VBZ'),\n",
       " ('that', 'IN'),\n",
       " ('portfolio', 'NN'),\n",
       " ('managers', 'NNS'),\n",
       " ('expect', 'VBP'),\n",
       " ('further', 'JJ'),\n",
       " ('declines', 'NNS'),\n",
       " ('in', 'IN'),\n",
       " ('interest', 'NN'),\n",
       " ('rates', 'NNS'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lemmasim(c[0], interestGoldData[0][2], \"res\")\n",
    "g = interestGoldData[0][2]\n",
    "wn.lemmas(g[2][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on yields\n",
      "[Lemma('on.a.01.on'), Lemma('on.a.02.on'), Lemma('along.r.01.on'), Lemma('on.r.02.on'), Lemma('on.r.03.on')] []\n"
     ]
    }
   ],
   "source": [
    "test_l1 = g[1][0]\n",
    "test_l2 = c[0][0]\n",
    "print(test_l1, test_l2)\n",
    "print(wn.lemmas(test_l1), wn.lemmas(test_l2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disambiguate_interest(inst) :\n",
    "    outsyns = inst[0]\n",
    "    position = inst[1]\n",
    "    context = inst[2]   # [('yields', 'NNS'), ('on', 'IN'), ('money-market', 'JJ'), ...]\n",
    "    \n",
    "    # Lösung A3 - kein plan...\n",
    "    \n",
    "#     p_dist = 9999\n",
    "#     for p in range(len(interestGoldData)):\n",
    "#         act_dist = abs(position - p)\n",
    "#         if  act_dist < p_dists:\n",
    "#             p_dist = act_dist\n",
    "#             outsyns = interestGoldData[i][0]\n",
    "#     for k in range(len(interestGoldData[]))\n",
    "#         for i in range(len(context)):\n",
    "#             lemmasim(context[i][0], interestGoldData[]\n",
    "    return outsyns   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Test für A3\n",
    "\n",
    "sumTrueDisambiguations = 0\n",
    "sumFalseDisambiguations = 0\n",
    "\n",
    "for i in range(len(interestTestData)) :\n",
    "    if disambiguate_interest(interestTestData[i]) == interestGoldData[i][0] :\n",
    "        sumTrueDisambiguations += 1\n",
    "    else :\n",
    "        sumFalseDisambiguations += 1\n",
    "\n",
    "accuracyDisambiguations = sumTrueDisambiguations /(sumTrueDisambiguations + sumFalseDisambiguations)\n",
    "\n",
    "print(\"Accuracy: {}\".format(accuracyDisambiguations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
